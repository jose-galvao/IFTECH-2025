{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af68f65e",
   "metadata": {},
   "source": [
    "# IFTECH 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30b2e83",
   "metadata": {},
   "source": [
    "## Estrutura simples para usar a API da OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaa50a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55ad2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=\"Fale alguma coisa interessante sobre o dia de hoje.\"\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43c844a",
   "metadata": {},
   "source": [
    "## Diferentes modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46d707f",
   "metadata": {},
   "source": [
    "gpt-4.1\t\n",
    "\n",
    "gpt-4.1-mini\n",
    "\n",
    "gpt-4.1-nano\n",
    "\n",
    "gpt-4o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff9a994",
   "metadata": {},
   "source": [
    "### Diferentes modelos tem valores diferentes de input e output. Cada modelo pode ser utilizado para funções diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bc40b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=\"Fale alguma coisa interessante sobre o dia de hoje.\"\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808e145f",
   "metadata": {},
   "source": [
    "## Roles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ca9f2f",
   "metadata": {},
   "source": [
    "Os papéis disponiveis no chatGPT desempenham funções diferentes:\n",
    "\n",
    "O system pode ser usado para indicar o tom da mensagem, a caracteristica do prompt.\n",
    "\n",
    "O user indica que o usuário está enviando a mesagem e ela deve ser levada em consideração pra obter a resposta. É o próprio prompt.\n",
    "\n",
    "O assistent é utilizado para manter o contexto na hora de utilizar a API de forma interativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14bb475",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    {'role' : 'system', 'content' : 'Você vai responder as perguntas de forma humorada, fazendo piadinhas.'}\n",
    "    {'role' : 'user', 'content' : 'Quantas estrelas existem aproximadamente na galáxia observável?'}\n",
    "    {'role' : 'assistant', 'content' : 'No universo observável, estima-se que haja aproximadamente 10^22 a 10^24 estrelas.'}\n",
    "    {'role' : 'user', 'content' : 'Quantas estão na via lactea?'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a7ff74",
   "metadata": {},
   "source": [
    "## Outros parâmetros interessantes de serem utilizados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb3ae4b",
   "metadata": {},
   "source": [
    "temperature: A temperatura vai de 0 a 1 e indica o quão deterministica a resposta vai ser. O número 0 indica uma resposta mais deterministica e o número 1 indica uma resposta mais criativa\n",
    "\n",
    "max_tokens: Esse parâmetro é utilizado para inidicar o tamanho máximo da resposta que você pode receber. Ele é bom pra controlar quantos tokens você vai usar e ter um controle maior de quanto você vai gastar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93a06e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    temperature=0.9\n",
    "    max_tokens=30  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b9fe23",
   "metadata": {},
   "source": [
    "Resposta mais criativa (0.9) e muito curta (30 tokens no máximo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a335b434",
   "metadata": {},
   "source": [
    "## Código completo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae4e977",
   "metadata": {},
   "source": [
    "Exemplo de código completo interativo com os 3 roles, com temperatura e max_token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e454792b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "#Criação do objejto cliente que é utilizado para enviar solicitações a API.\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "#Lista de dicionário que vai guardar o historico da conversa.\n",
    "prompt = [\n",
    "    {'role': 'system', 'content': 'Você vai responder as perguntas de forma humorada, fazendo piadinhas.'},\n",
    "]\n",
    "\n",
    "\n",
    "while True:\n",
    "    #Input do prompt do usuário\n",
    "    input_prompt = input('Digite... ')\n",
    "    #Se for digiatado a palavra 'fim' o programa acaba.\n",
    "    if input_prompt.lower() == 'fim':\n",
    "        break\n",
    "#Caso contrário, adiciona o que foi digitado na mensagem do usuário (content) que é adicionado na lista prompt\n",
    "    else:\n",
    "        prompt.append({'role': 'user', 'content': input_prompt})\n",
    "#Faz a chamada da API para receber a resposta. Local onde são adicionados os parâmetros: model, messages, temperature e max_tokens\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=prompt,\n",
    "            temperature=0,\n",
    "            max_tokens=100\n",
    "        )\n",
    "#Pega o conteúdo gerado pelo modelo.\n",
    "        resposta = response.choices[0].message.content #A API pdoe gerar várias respostas (choices), mas estamos utilizando o indice 0 para pegar a primeira resposta.\n",
    "        #A resposta é adicinada ao prompt pra manter o contexto.\n",
    "        prompt.append({'role': 'assistant', 'content': resposta})\n",
    "\n",
    "        print('Resposta:', resposta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
